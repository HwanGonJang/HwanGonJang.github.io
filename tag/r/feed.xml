<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://hwangonjang.github.io/tag/r/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://hwangonjang.github.io/" rel="alternate" type="text/html" />
  <updated>2021-11-28T13:27:39+09:00</updated>
  <id>https://hwangonjang.github.io/tag/r/feed.xml</id>

  
  
  

  
    <title type="html">Gon’s Portfolio | </title>
  

  
    <subtitle>장환곤의 포트폴리오 사이트</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">R과 구글 트렌드를 활용한 코로나 관련 검색어 탐구</title>
      <link href="https://hwangonjang.github.io/r_1" rel="alternate" type="text/html" title="R과 구글 트렌드를 활용한 코로나 관련 검색어 탐구" />
      <published>2021-08-14T01:40:00+09:00</published>
      <updated>2021-08-14T01:40:00+09:00</updated>
      <id>https://hwangonjang.github.io/r_1</id>
      <content type="html" xml:base="https://hwangonjang.github.io/r_1">&lt;h1 id=&quot;r과-구글-트렌드를-활용한-코로나-관련-검색어-탐구&quot;&gt;R과 구글 트렌드를 활용한 코로나 관련 검색어 탐구&lt;/h1&gt;

&lt;h2 id=&quot;1-개요&quot;&gt;1. 개요&lt;/h2&gt;

&lt;p&gt;확률과 통계 과목을 통해 배운 지식과 도구들로 R과 구글 트렌드를 활용한 코로나 관련 검색어 탐구를 진행합니다. 탐구 주제는 구글트렌드로 알아보는 대한민국의 코로나 바이러스 실태와 실제 국민들은 코로나 바이러스를 어떻게 인식하고 있는지, 정말 전세계적 판데믹 상황으로 받아들이는지 약 1년 반 동안의 휴식으로 인식하는지 또는 어떤 생각을 하고 있는지 연령대 별로 통계를 내서 알아보도록 하겠습니다. 이외에도 네이버 데이터랩의 검색 데이터를 활용하여 검증을 거치도록 하겠습니다. 이때 확률과 통계 과목에서 배운 지식과 통계 기술을 사용하고, 도구로는 R, 엑셀 등을 활용하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;이때, 구글 트렌드와 네이버 데이터 랩은 포털 사이트 구글, 네이버에서 이용자들이 검색한 검색어들을 공개해주는 서비스입니다. ‘모두 거짓말을 한다’라는 책에서는 포털 사이트의 검색창은 모든 사람이 자신의 걱정 혹은 생각들을 숨김없이 드러내는 곳이라고 합니다. 가족 심지어 자신에게 까지 드러내지 못한 감정이나 생각들을 포털 사이트에는 검색하여 정보를 얻는 일이 많기 때문입니다. 이러한 심리를 가설로 설정하고 실제로 국민들이 코로나 바이러스를 어떻게 인식하고 있는지 연구하도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;2-구글트렌드를-통해-검색량-데이터-가져오기&quot;&gt;2. 구글트렌드를 통해 검색량 데이터 가져오기&lt;/h2&gt;

&lt;h3 id=&quot;2-1-통계-데이터-가져오기&quot;&gt;2-1 통계 데이터 가져오기&lt;/h3&gt;

&lt;p&gt;구글 트렌드는 구글에서 제공하는 특정 검색량에 대한 추이 및 통계입니다. 사용자는 원하는 검색어를 구글트렌드에 입력하면 그 검색어에 대한 원하는 기간에서의 검색량을 csv 파일로 제공해줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/HwanGonJang/HwanGonJang.github.io/blob/master/Pictures/st_2-1.png?raw=true&quot; /&gt;
(사진 1.구글 트렌드 ‘코로나’ 검색어 입력 사진)&lt;/p&gt;

&lt;p&gt;이번 Term-Paper에서는 이렇게 얻은 검색어에 대한 csv파일을 이용하기 쉽게 엑셀 파일(xlsx)로 변환하여 R Studio 에서 통계 분석을 시행합니다. 통계에 대한 분석 결과를 통해 코로나 바이러스가 우리에게 미친 영향, 코로나 바이러스에 대한 인식의 변화 등을 탐구 하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/HwanGonJang/HwanGonJang.github.io/blob/master/Pictures/st_2-2.png?raw=true&quot; /&gt;
(사진2. 구글 트렌드 ‘코로나’ 검색어 엑셀 파일)&lt;/p&gt;

&lt;h3 id=&quot;2-2-r-studio-엑셀-가져와서-출력하기&quot;&gt;2-2 R studio 엑셀 가져와서 출력하기&lt;/h3&gt;
&lt;p&gt;위의 엑셀 파일을 이용하여 R Studio 에서 분석을 진행합니다. 엑셀 데이터를 R에서 다시 그리고 이 데이터를 활용하여 시각화를 합니다.&lt;/p&gt;

&lt;p&gt;먼저 엑셀 파일을 R Studio 프로젝트 폴더에 업로드 합니다.
Home\statisticAssignment 경로로 프로젝트 폴더를 만들고 엑셀 파일을 옮겨줍니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;install.packages(“readxl”)  &lt;br /&gt;
//엑셀 파일을 다루기 위해 readxl 라이브러리 다운로드
library(readxl)  &lt;br /&gt;
//readxl 라이브러리 로드
df1_exam_corona &amp;lt;- read_excel(“coronaExcel.xlsx”)  &lt;br /&gt;
//엑셀 파일을 읽을 변수 생성
df1_exam_corona
//엑셀 파일의 데이터 가져오기&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위와 같은 과정으로 엑셀 파일의 데이터를 가져와 R Studio 에 그릴 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/HwanGonJang/HwanGonJang.github.io/blob/master/Pictures/st_2-3.png?raw=true&quot; /&gt;
(사진3. R Studio에 엑셀 데이터 그리기)&lt;/p&gt;

&lt;h3 id=&quot;2-3-r-studio-엑셀-데이터-시각화하기&quot;&gt;2-3 R studio 엑셀 데이터 시각화하기&lt;/h3&gt;
&lt;p&gt;df1_exam_corona 변수로 ‘코로나’ 검색어에 대한 통계 파일을 받아왔습니다. 이제 이 파일을 시각화하기 위해 ggplot2 라는 패키지를 다운로드 받습니다. ggplot2 는 R의 대표적인 그래프 그리기 툴입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;install.packages(“ggplot2”)  &lt;br /&gt;
//테이블의 시각화를 위해 ggplot2 라이브러리 다운로드
library(ggplot2)  &lt;br /&gt;
//ggplot2 라이브러리 로드
df1_exam_corona &amp;lt;- read_excel(“coronaExcel.xlsx”)  &lt;br /&gt;
//엑셀 파일을 읽을 변수 생성
ggplot(data=df1_exam_corona, aes(x=Day, y=Corona, group=1)) + geom_line()
//엑셀 파일의 데이터를 통해 선그래프 그리기&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;검색어 통계를 보기에 가장 직관적인 선 그래프를 그리기 위해 geom_line()을 통해 선 그래프를 그려주었습니다. 이것으로 ‘코로나’ 검색어에 대한 통계를 가져와 그래프를 그리는 것까지의 과정을 완료했습니다. 지금부터는 ‘코로나’ 이외의 검색어를 구글트렌드로 같은 방법으로 가져와 분석하도록 하겠습니다.
 &lt;/p&gt;
&lt;h3 id=&quot;3-1코로나검색량-통계&quot;&gt;3-1.‘코로나’검색량 통계&lt;/h3&gt;
&lt;p&gt;#### 3-1-1 통계 및 정리&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/HwanGonJang/HwanGonJang.github.io/blob/master/Pictures/st_2-4.png?raw=true&quot; /&gt;
(사진4. R Studio ‘코로나’ 검색어 통계 선 그래프)&lt;/p&gt;

&lt;p&gt;2 의 과정을 거쳐 R Studio로 검색량 추이 통계 선 그래프를 그리는데 성공했습니다. 위의 그래프는 2019.12.01 ~ 2021.06.06 까지의 코로나 검색어의 검색량 통계입니다. 위 그래프를 통해 코로나 바이러스가 창궐한 2020년 초 코로나 바이러스에 대한 관심이 급증하여 사람들이 코로나 바이러스에 대해 위험 인식을 가지고 경계 하였음을 알 수 있습니다. 그러나 약 1년 후 마스크와 비대면 생활에 익숙해진 사람들은 코로나 바이러스에 대한 관심(검색)이 상대적으로 줄었다는 것을 알 수 있습니다. 
한가지 재미있는 것은 시기입니다. 대한민국은 지금까지 3번의 대유행이 있었다. 2020년 3월 신천지에의한 1차 대유행, 2020년 8월 성북구 교회에 의한 2차 대유행, 또, 같은 해 11월 방역 한계와 무증상 감염자에 의한 3차 대유행입니다. 위의 코로나 검색어 그래프는 대한민국의 코로나 바이러스 1, 2, 3차 대유행 기간에 따라 검색어 그래프가 상승 곡선을 그린다는 것도 알 수 있습니다. 따라서 이러한 결과를 통해 코로나 바이러스에 대한 경계의 정도와 관심을 알 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;3-2-휴일-휴식-관련-검색량-통계&quot;&gt;3-2. 휴일, 휴식 관련 검색량 통계&lt;/h3&gt;
&lt;p&gt;#### 3-2-1 ‘숙박’ 검색어 통계 및 정리&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/HwanGonJang/HwanGonJang.github.io/blob/master/Pictures/st_2-5.png?raw=true&quot; /&gt;&lt;/p&gt;

&lt;p&gt;코로나 검색어 통계와 같은 과정을 거쳐 ‘숙박’이라는 검색어에 대한 구글트렌드 통계를 분석했습니다. 휴일에 나가고 싶어하는 사람들의 심리는 당연한 것이지만 코로나 바이러스 판데믹 상황에서는 자제해야 한다는 걸 모두 알고 있습니다. 따라서 휴일 관련 통계는 사람들의 바이러스 인식과 심리를 잘 나타내 줄 것이라는 가설을 세웠고 이는 꽤 들어 맞았습니다.
 ‘숙박’이라는 검색어에 대한 통계 그래프는 몇가지 흥미로운 점을 시사합니다. 첫번째로 2019년과 2020년의 경계입니다. 바이러스 창궐 전 2019년 말까지 숙박 검색량을 볼 때 많은 사람들이 여행 계획을 가지고 있었음을 알 수 있습니다. 특히 겨울방학 시점에 최고점을 찍은 것을 알 수 있습니다. 그러나 바이러스 창궐 이후 숙박, 즉 여행에 대한 관심이 크게 떨어졌습니다. 바이러스에 대한 경계심 때문입니다. 이를 통해 사람들이 바이러스에 대해 조심하고 있음을 알 수 있습니다. 
두번째로 흥미로운 점은 그래프의 상승 곡선 시기입니다. 3-1에서 다룬 바와 같이 대한민국은 3번의 대유행이 있었습니다. 3월, 8월, 11월 입니다. 놀라운 점은 그래프가 3월, 8월 11월 시기에 상승곡선을 형성한다는 것입니다. 사람들은 대유행 직전 바이러스에 대한 경계가 헤이해졌고 많은 사람들이 참아왔던 여행을 감행한 것으로 이해할 수 있습니다. 물론 교회 발 확산이라는 사실이 가장 큰 이유이겠지만 사람들의 여행에 대한 관심의 급증도 또 하나의 이유가 될 수 있음을 시사하는 그래프입니다.&lt;/p&gt;

&lt;h3 id=&quot;3-3-줌-등-온라인-기능-이용-통계&quot;&gt;3-3. 줌’ 등 온라인 기능 이용 통계&lt;/h3&gt;
&lt;p&gt;#### 3-3-1‘줌’검색어 통계 및 정리&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/HwanGonJang/HwanGonJang.github.io/blob/master/Pictures/st_2-6.png?raw=true&quot; /&gt;&lt;/p&gt;

&lt;p&gt;코로나 바이러스가 창궐한 이후 가장 큰 영향을 받은 분야는 분명 교육입니다. 바이러스 확산을 막기 위해 초중고대학교들은 모두 비대면 수업을 강행했고 수업을 위해 줌이나 구글 클래스룸 기능을 이용했습니다. 따라서 학교를 다니는 1020세대의 학생들은 이 둘을 자주 이용할 것입니다. 또한 학생 이외에도 직장인과 같은 사회인 들도 비대면 미팅 프로그램을 자주 이용할 것입니다.
위 그래프는 대표적인 온라인 미팅 프로그램 줌에 대한 구글의 검색량 통계를 그래프로 그린 것입니다. 2019년 까지 거의 이용 및 검색이 없었던 프로그램인 줌은 2020년 코로나 바이러스 창궐 이후 폭발적으로 이용량이 증가했음을 알 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;3-4백신검색량-통계&quot;&gt;3-4.‘백신’검색량 통계&lt;/h3&gt;
&lt;p&gt;#### 3-4-1‘백신’검색량 통계 및 정리&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/HwanGonJang/HwanGonJang.github.io/blob/master/Pictures/st_2-7.png?raw=true&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2021년으로 판데믹 상황이 1년 이상 지속된 후 화이자, 아스트로제네카 등 기업에서 백신을 개발했습니다. 개발한 백신으로 현재 미국, 유럽 등은 꽤 높은 접종률을 기록했고 많은 사람들이 마스크 없는 일상을 되찾았습니다. 이는 곧 대한민국의 일상도 되찾아 줄 수 있을 것입니다. 따라서 순차적으로 모두가 백신을 맞는 것이 중요합니다. 
위 백신에 대한 검색어 통계 그래프를 통해 사람들의 백신 관심도가 크게 올랐음을 알 수 있습니다. 또한 백신 검색어는 곧 백신 접종 예약을 위한 검색으로 이어지기도 하기 때문에 좋은 현상이라고 볼 수 있을 것 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;4-결론&quot;&gt;4. 결론&lt;/h2&gt;
&lt;p&gt;이렇게 많은 사람들의 일상과 행복을 빼앗은 코로나 바이러스 판데믹 상황에 대해 R을 활용하여 검색어의 검색량을 기준으로 분석을 해보았습니다. 확률과 통계 과목에서 배운 R Programming을 활용하여 간단하지만 의미있는 통계 분석을 했습니다. 
     검색량을 분석하는 것은 굉장히 의미있었습니다. 검색이라는 수단은 사람들로 하여금 자신의 의도와 속을 잘 드러낼 수 있는 방법이었습니다. 따라서 코로나 바이러스라는 민감한 주제에도 검색 만큼은 자신이 판데믹 상황에도 불구하고 하고 싶은 것 혹은 할 것 들을 가감없이 드러냈습니다. 그러한 결과인 검색량은 굉장히 신빙성 있는 통계였습니다. 이를 그래프로 그려 판단해보았을 때에도 실제 한국의 상황과 잘 들어맞음을 알 수 있었습니다.&lt;/p&gt;

&lt;p&gt;검색어를 분석하는 것 외에도 R Programming은 데이터의 질과 분석가의 역량에 따라 무궁한 잠재력을 갖고 있습니다. 이번 텀페이퍼를 통해 가치있는 분석을 도출하기 위해 확률과 통계 과목의 기초는 굉장히 중요함을 알 수 있었습니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>HwanGonJang</name>
        
        
      </author>

      

      
        <category term="r" />
      

      
        <summary type="html">R과 구글 트렌드를 활용한 코로나 관련 검색어 탐구</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">An Empirical Study of Software Reuse with Special Attention to Ada 논문 리뷰</title>
      <link href="https://hwangonjang.github.io/st_1m" rel="alternate" type="text/html" title="An Empirical Study of Software Reuse with Special Attention to Ada 논문 리뷰" />
      <published>2021-06-15T01:40:00+09:00</published>
      <updated>2021-06-15T01:40:00+09:00</updated>
      <id>https://hwangonjang.github.io/st_1m</id>
      <content type="html" xml:base="https://hwangonjang.github.io/st_1m">&lt;h1 id=&quot;an-empirical-study-of-software-reuse-with-special-attention-to-ada-논문-리뷰&quot;&gt;An Empirical Study of Software Reuse with Special Attention to Ada 논문 리뷰&lt;/h1&gt;

&lt;h2 id=&quot;개요&quot;&gt;개요&lt;/h2&gt;

&lt;p&gt;숭실대학교 이남용 교수님의 “An Empirical Study of Software Reuse with Special Attention to Ada” 논문은 Ada에 집중한 소프트웨어 재사용의 실증적 연구 논문입니다. 논문의 전체적인 내용과 Ada는 무엇인지 설명하고 리뷰를 진행합니다.&lt;/p&gt;

&lt;h3 id=&quot;1-머리말&quot;&gt;1. 머리말&lt;/h3&gt;

&lt;p&gt;확률과 통계 과목 중간시험으로 “An Empirical Study of Software Reuse with Special Attention to Ada” 논문을 읽고 이해한 것을 바탕으로 에세이를 작성합니다. 이남용 교수님의 “An Empirical Study of Software Reuse with Special Attention to Ada” 논문은 Ada에 집중한 소프트웨어 재사용의 실증적 연구 논문입니다. 따라서 에세이를 시작하기 전 논문의 전체적인 내용과 Ada는 무엇인지 설명하도록 하겠습니다.&lt;/p&gt;

&lt;h4 id=&quot;1-1-ada&quot;&gt;1-1. Ada&lt;/h4&gt;

&lt;p&gt;이 논문은 소프트웨어 재사용 기술을 조사하고 검토하여 실제로 조직이 재사용 기술을 어떻게 활용하는지 분석하고 소프트웨어 재사용에 영향을 미치는 요인을 평가합니다. 이때, Ada 기술을 이용하는 조직의 재사용성을 조사하여 재사용성의 효과성과 수준을 측정하는 기술을 개선합니다. Ada는 1970년대에 소프트웨어 비용을 줄이기 위해 설계되어 미국 국방부 표준으로 선정되었습니다. Ada는 상대적으로 빠르게 구현 할 수 있고 재사용을 촉진하는 언어입니다. 이러한 언어는 객체지향 설계 방법론과 함께 사용 가능하며, 이식성이 높아 재사용성을 검증하는 데에 좋은 도구입니다.&lt;/p&gt;

&lt;h3 id=&quot;2-explaining-article&quot;&gt;2. Explaining Article&lt;/h3&gt;

&lt;h4 id=&quot;2-1-how-to-collect-the-random-sample-data&quot;&gt;2-1. How to collect the random sample data&lt;/h4&gt;

&lt;p&gt;무작위 표본 데이터를 수집하는 방법을 설명합니다. 단순 무작위 표본 추출 기법은 주요 확률 표본 추출 기법 중 하나로 편향이 적고 일반화 가능성이 높다는 것이 특징입니다. 단순 무작위 표본은 모집단의 무작위하게 선택된 부분집합입니다. 이 표본 추출 방법에서 모집단의 각 성분은 선택될 확률이 같습니다. 이 방법은 모든 확률 표본 추출 방법 중 가장 간단한 방법입니다. 이 방법은 오직 단일 무작위 선택을 포함하고 모집단에 대한 지식이 거의 필요하지 않기 때문입니다.&lt;/p&gt;

&lt;p&gt;이 단순 무작위 표본 추출 방법은 모집단에 대한 통계적 추론을 만드는 데 사용됩니다. 이는 내부 유효성을 보장하는 데 도움이 됩니다. 무작위화는 잠재적으로 헷갈리게 할 수 있는 변수의 영향을 줄일 수 있는 가장 좋은 방법이기도 합니다. 또한 충분히 큰 표본은 외부 타당성이 높습니다. 하지만 간단한 무작위 표본 추출은 실제로 구현하기 어렵습니다. 이 방법을 사용하기 위해서는 표본에 대한 전체 정보가 있어야 합니다. 또, 모집단이 선택 되면 접근 할 수 있어야 하며, 필요한 표본 크기에서 데이터를 수집할 시간과 리소스가 있어야 합니다.&lt;/p&gt;

&lt;p&gt;이러한 조건 아래에서 우리는 Random Sample data, 무작위 표본 데이터를 수집할 수 있습니다.&lt;/p&gt;

&lt;p&gt;첫번째 단계로는, 모집단을 정의하는 것입니다. 연구할 모집단을 결정하는 것으로 무작위 표본 추출을 시작합니다. 표본을 위해 선택된 모든 데이터를 수집할 수 있도록 모집단의 개별 구성원에 접근 할 수 있는지 확인합니다.&lt;/p&gt;

&lt;p&gt;두번째 단계는 표본 크기 결정입니다. 큰 표본은 높은 통계적 확실성을 제공하지만&lt;/p&gt;

&lt;p&gt;비용이 높고 많은 작업이 필요합니다. 여러가지 방법 중 하나는 신뢰 구간 및 신뢰 수준, 표준 편차로 계산하는 것입니다.&lt;/p&gt;

&lt;p&gt;세번째 단계는 표본의 무작위 선택입니다. 이 방법으로는 추첨과 랜덤의 두가지 방법이 있습니다. 추첨 방법은 동일 작업을 시뮬레이션하는 프로그램을 사용하여 무작위로 표본을 선택하는 것입니다. 랜덤 방법은 숫자를 할당하고 그 다음 난수 생성기나 표를 이용하여 모집단의 부분 집합을 무작위로 선택합니다.&lt;/p&gt;

&lt;p&gt;마지막으로 데이터를 표본으로부터 수집합니다. 이때, 의 유효성을 보장하려면 선택된 모든 정보가 실제로 연구에서 사용되고 있는지 확인해야 합니다.&lt;/p&gt;

&lt;p&gt;#### 2-2. How to determine the random sample size&lt;/p&gt;

&lt;p&gt;단순 무작위 표본에서 표본의 크기를 정하는 것은 매우 중요합니다. 표본 데이터를 수집하는 방법에서 다루었듯이 표본의 크기는 비용과 작업량에 큰 영향을 주기 때문에 용도에 맞게 크기를 결정하는 것이 중요합니다.&lt;/p&gt;

&lt;p&gt;표본 크기는 조사 또는 실험을 위해 선택된 모집단의 일부입니다. 통계에서 표본 크기를 찾는 방법은 계산기 혹은 공식을 이용하는 것입니다. 우선 모집단이 적은 경우에 모집단 조사를 실행합니다. 첫번째 방법은 유사한 연구의 표본 크기를 사용하는 방법이 있습니다. 두번째 방법은 표를 사용하는 것입니다. 일반적인 주제라면 이미 연구된 테이블이 있을 것입니다. 세번째 방법은 표본 크기 계산기입니다. 복잡하고 전문적인 시중의 계산기를 활용 가능합니다. 마지막으로 수식을 이용하는 방법입니다. 모집단에 대해 알고 있는 변수에 따라 다양한 공식을 사용할 수 있습니다. 기본적으로 표준 편차를 알고 있다면 Cochran의 표본 크기 공식을 활용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Cochran의 표본 크기 공식을 사용하면 정밀도 수준, 신뢰 수준 및 모집단에 존재하는 속성의 비율을 고려하여 이상적인 표본 크기를 계산할 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;2-3-research-instrument-and-measurement-scale&quot;&gt;2-3. Research instrument, and measurement scale&lt;/h4&gt;

&lt;p&gt;이 연구의 방법으로는 메일 조사를 선택했습니다. 메일 조사는 이 연구의 인구가 지리적으로 분산되었을 때 저비용인 방법이기 때문입니다. 이 연구의 표본들은 재사용 및 Ada 기술에 대한 경험을 갖고 있습니다. 이것들은 Ada 기술과 재사용에 익숙한 ACM Special Interest Group on Ada(SIGADA) 멤버입니다. Ada 메일링 리스트에서 AdaIC Electronic Bulletin Board, AdaIC Newsletters and the ACM Ada Letters 500명이 표본으로 선택되었습니다. 이에 대한 조사로 본 연구의 목적을 설명하고 참여를 요청하는 자기소개서와 함께 주요 용어의 정의가 포함된 1페이지 분량의 설문지가 개발되었습니다. 문지는 조사 질문과 관련된 37개의 질문으로 구성되었습니다. 이 질문들 중, 19개의 질문만이 이 연구에서 사용되었습니다.&lt;/p&gt;

&lt;h3 id=&quot;3-discussing-the-statistical-techniques&quot;&gt;3. Discussing the statistical techniques&lt;/h3&gt;

&lt;p&gt;3번에서는 이 연구에 사용된 통계적 기술들을 이야기합니다.&lt;/p&gt;

&lt;p&gt;#### 3-1. Homogeneity test&lt;/p&gt;

&lt;p&gt;본 연구에서 사용된 통계적 기술로 균질성 테스트가 있습니다. 본 연구에서는 다양한 공간을 사용할 수 있지만 한 페이지의 설문 조사의 한계 때문에 소프트웨어 크기만을 고려했습니다. 비응답 편향은 설문지의 소프트웨어 측정 기준인 조직에서 사용하는 소프트웨어 크기를 사용하여 조사되었습니다. 대표적으로 Chisquare 적합도 검정이 있습니다. 이러한 소프트웨어의 두 가지 분포가 무응답 편향 테스트를 위해 사용되었습니다. 첫 번째는 응답자들이 고용된 조직에서 소프트웨어 크기의 배포입니다. 다른 하나는 Ada의 Ada Usage Database가 관리하는 500개 조직의 소프트웨어 크기입니다. 모집단과 표본 피실험자를 비교하기 위해 카이제곱 적합도 검정을 실행합니다. 계산된 카이제곱 값이 11.90이고 카이제곱 테이블 값보다 작으며 D = 0.05에서 6의 자유도가 있으므로, 저자는 표본과 모집단 분포가 동일하다고 결론을 내렸습니다. 즉, 소프트웨어 크기에 따라 표본에 명백한 비응답 편향이 없습니다. 따라서 응답자 데이터에 기초한 본 연구의 통계적 추론이 뒷받침 됩니다.&lt;/p&gt;

&lt;h4 id=&quot;3-2-reliability-test&quot;&gt;3-2. Reliability test&lt;/h4&gt;

&lt;p&gt;데이터 분석에는 4가지 단계가 있습니다. 첫번째 단계는 응답자 중 한명의 특성에 기초한 비응답 편향 테스트를 사용하여 데이터의 대표성을 확인하는 것입니다. 두번째 단계가 연구 기구의 신뢰성과 유효성을 검토하는 것입니다.&lt;/p&gt;

&lt;p&gt;신뢰성은 광범위하게 측정값에 오류가 없을 정도로 정의될 수 있기 때문에 일관된 결과를 얻을 수 있습니다. Cronbach의 알파 계수의 Thumb 기준은 0.70 이상입니다. Ada 경험과 Ada 툴의 성숙도는 신뢰성이 상대적으로 높은데, 본 연구에서 재사용 경험, 저장소의 성숙도, 전무의 관심사, 재사용 프로그램의 수준, 재사용 엔지니어 수의 변수 그룹에 대한 알파 값인 0.84는 연구의 측정에 높은 신뢰성을 보여줍니다. 또한 미국 DoD 표준의 영향, ANSI 표준의 영향, ISO 표준의 영향, Ada 소프트웨어 구성 요소의 증가하는 수요의 영향 등의 변수 그룹에 대한 알파 값인 0.91도 이러한 측정에서 높은 신뢰성을 나타냅니다. 연구에서 모든 알파 값이 0.70 보다 크기 때문에 본 연구의 측정은 어느 정도 신뢰 할 수 있다고 결론지을 수 있습니다. 그리고 이어서 특성의 유효성 검사를 진행합니다.&lt;/p&gt;

&lt;h4 id=&quot;3-3-validity-test&quot;&gt;3-3. Validity test&lt;/h4&gt;

&lt;p&gt;신뢰성 검사에 이어 특성 유효성 검사를 위해 주성분 분석을 수행합니다. 유효성은 개념, 결로 SEHS 측정이 근거가 있고 실제와 정확히 일치할 가능성을 검사하는 것입니다. 본 연구에서는 고유값과 scree tail 검사를 이용하여 여러 요인을 추출합니다.&lt;/p&gt;

&lt;p&gt;스크리테일 테스트에서는 추출 순서에서 인자 수에 대한 잠재 루트를 플로팅하여 도출하고, 결과 곡선의 모양을 사용하여 곡선이 처음 직선화되기 시작하는 지점을 추출할 최대 요인의 수를 나타내는 것으로 간주합니다. 일반적으로 요인의 고유값이 1.0보다 크면 추가 분석을 위해 요인을 유지해야 합니다. 이 연구에서는 7개 요인의 고유값이 1.0보다 큽니다. 따라서 이 연구의 요인 수는 7개로 제한되었습니다. 따라서 변수 수를 줄이기 위해 Varimax method를 사용했습니다. 요인 하중은 원래 변수와 요인 간의 상관 관계이며 특정 요인의 특성을 이해하는 열쇠입니다. 요인 하중의 제곱은 인자에 의해 설명되는 원래 변수의 분산 백분율을 나타냅니다. 요인의 유의성에 대한 엄지의 법칙은 0.5보다 크거나 같습니다. 변수는 유의한 교차 로딩을 나타냅니다. 즉, 요인 내 두 개 이상의 변수가 0.3보다 큽니다. 그러나 이러한 각 변수는 요인 간의 교차 적재에서 더 큰 차이를 보입니다. 따라서 이 변수들은 높은 판별 유효성을 가지고 있다는 결론이 나옵니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/HwanGonJang/HwanGonJang.github.io/blob/master/Pictures/ds_8.png?raw=true&quot; /&gt; 
*는 정밀도 수준, p는 모집단의 추정 비율, q = p-1 입니다. *&lt;/p&gt;

&lt;h4 id=&quot;3-4-descriptive-statistics&quot;&gt;3-4. Descriptive statistics&lt;/h4&gt;

&lt;p&gt;본 연구에서 Descriptive statistics 기술 통계량은 응답자들이 재사용 속도의 표준화 정책을 어떻게 인식하고 있는지 이해하기 위해 사용되었습니다.&lt;/p&gt;

&lt;p&gt;기술 통계량은 주어진 데이터 세트를 요약하는 간단한 기술 계수로 전체 또는 모집단 표본 일 수 있습니다. 기술 통계량은 중심 경향 측정과 변동성 측정으로 나뉩니다. 중심 경향 측정에는 평균, 중앙값, 최빈값이 포함되고, 변동성 측정 값에는 표준편차, 분산 등이 포함됩니다. 모든 기술 통계량은 중심 경향 측정 값 또는 변동성 측정 값입니다. 본 연구에서는 분산, 표준편차에 중심을 둔 변동성 측정 값을 활용했습니다. 변동성 측정 값은 데이터 집합에 대한 분포를 분석하는데 도움이 됩니다. 이는 데이터 세트의 모양과 분포를 설명하기 때문에 세트 내에서 데이터가 분산되는 방식을 설명할 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;3-5-factor-analysis&quot;&gt;3-5. Factor Analysis&lt;/h4&gt;

&lt;p&gt;본 연구에서는 가설을 테스트 하기 위해 인자 분석(Factor Analysis)과 회귀 분석을 활용합니다.&lt;/p&gt;

&lt;p&gt;우선 인자 분석은 잠재적으로 적은 숫자의 관찰되지 않은 변수인 인자들로 관찰된 변수들 사이에서 분산을 설명하기 위한 통계학적 방법입니다. 이 기술의 목적은 많은 수의 변수들 간의 상호 관계를 분석하고 이러한 변수들을 공통의 기본 치수 또는 인자의 관점에서 설명하는 것입니다.&lt;/p&gt;

&lt;p&gt;인자 분석에서 데이터 세트의 인자를 추출하는 데에는 여러가지 방법이 있습니다. 첫째, 주성분 분석입니다. 가장 일반적인 방법으로 최대 분산을 추출하여 첫째 요인에 넣고 이로인해 설명된 분산을 제거한 후 다음 과정을 진행합니다. 두번째, 공통 인자 분석입니다. 공통 분산을 추출하여 인자에 넣는 방법입니다. 세번째, 이미지 인자화입니다. 행렬을 기반으로 하는 인자를 예측하는데 사용됩니다. 이외에 여러가지 인자 분석 방법이 존재합니다.&lt;/p&gt;

&lt;h4 id=&quot;3-6-surrogate-variables&quot;&gt;3-6. Surrogate variables&lt;/h4&gt;

&lt;p&gt;본 연구에서는 주성분 분석을 기반으로 7개의 대리 변수가 생성되었습니다. 이 변수들은 다중 회귀 분석에서 사용됩니다.&lt;/p&gt;

&lt;p&gt;대리 변수, Surrogate variables는 측정하기 힘든 변수들 대신 사용되는 측정할 수 있는 변수 입니다. 본 연구에서는 각 주제에 따른 독립 변수로 처리되는 7개의 대리 변수를 활용하여 회귀 방정식의 적합도를 측정합니다. 유의성 검정을 활용해 확인한 결과 대리 변수 7개 중 하나 이상이 종속 변수의 변동을 예측하기 유리함을 알 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;3-7-multiple-regression-analysis&quot;&gt;3-7. Multiple regression analysis&lt;/h4&gt;

&lt;p&gt;다중 회귀분석은 회귀분석의 종류 중 하나입니다. 다중 회귀분석은 단순 회귀분석의 단점을 보완한 방법입니다. 먼저 단순 회귀분석은 원인이 되는 독립 변수와 결과가 되는 종속 변수가 존재하는데 독립변수가 변함에 따라 종속변수가 얼마나 변하는지 보여줍니다. 그러나 이는 제 3변수의 영향을 통제하지 못하고 이 단점을 다중 회귀분석으로 보완합니다.&lt;/p&gt;

&lt;p&gt;다중 회귀분석은 단순 회귀분석에 독립변수들이 추가된 것입니다. 두 개 이상의 독립변수를 갖고 종속변수를 설명하거나 예측할 때 사용합니다. 하지만 독립 변수가 추가될수록 분석 그래프의 차원이 증가하여 4차원을 넘어서면 이해가 힘들어지고 계산이 복잡해집니다.&lt;/p&gt;

&lt;p&gt;즉, 다중 회귀분석은 다중 회귀식,  Y=β0+β1X1+β2X2+ … +βkXk(X: 독립변수, Y: 종속변수, β: 회귀계수, β0: Y절편, β1~βk: 독립변수의 기울기)의 모형의 유의성을 확인하는 것입니다. 다중 회귀분석은 변수의 개수가 같아도 포함된 변수에 따라 모형은 다르게 나타날 수 있으며, 여러 개의 변수 중에서 통계적으로 유의한 것만 선택할 수 있습니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>HwanGonJang</name>
        
        
      </author>

      

      
        <category term="r" />
      

      
        <summary type="html">An Empirical Study of Software Reuse with Special Attention to Ada 논문 리뷰</summary>
      

      
      
    </entry>
  
</feed>
